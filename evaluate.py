"""
MiniMind æ¨¡å‹è¯„ä¼°è„šæœ¬
æ”¯æŒå¤šç§è¯„ä¼°æŒ‡æ ‡ï¼šå›°æƒ‘åº¦(Perplexity)ã€å‡†ç¡®ç‡ã€æŸå¤±ç­‰
"""
import argparse
import json
import math
from pathlib import Path
from typing import Dict, List, Optional

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from tqdm import tqdm

from minimind import MiniMindConfig, MiniMindForCausalLM
from dataset import MiniMindTokenizerFast, MinimindDataset


class ModelEvaluator:
    """MiniMind æ¨¡å‹è¯„ä¼°å™¨"""
    
    def __init__(
        self,
        model_path: str,
        tokenizer_path: str,
        device: str = "cuda",
        batch_size: int = 4,
        max_seq_len: int = 512
    ):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            model_path: æ¨¡å‹æƒé‡è·¯å¾„
            tokenizer_path: åˆ†è¯å™¨è·¯å¾„
            device: è®¡ç®—è®¾å¤‡
            batch_size: æ‰¹æ¬¡å¤§å°
            max_seq_len: æœ€å¤§åºåˆ—é•¿åº¦
        """
        self.device = torch.device(device if torch.cuda.is_available() else "cpu")
        self.batch_size = batch_size
        self.max_seq_len = max_seq_len
        
        # åŠ è½½åˆ†è¯å™¨
        print(f"ğŸ“š åŠ è½½åˆ†è¯å™¨: {tokenizer_path}")
        self.tokenizer = MiniMindTokenizerFast.from_pretrained(tokenizer_path)
        
        # åŠ è½½æ¨¡å‹
        print(f"ğŸ¤– åŠ è½½æ¨¡å‹: {model_path}")
        self._load_model(model_path)
        
        # æŸå¤±å‡½æ•°
        self.criterion = nn.CrossEntropyLoss(reduction='none')
        
        print(f"âœ… è¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ (è®¾å¤‡: {self.device})")
    
    def _load_model(self, model_path: str):
        """åŠ è½½æ¨¡å‹æƒé‡"""
        # å°è¯•åŠ è½½é…ç½®
        config_path = Path(model_path).parent / "config.json"
        if config_path.exists():
            self.config = MiniMindConfig.from_pretrained(str(config_path.parent))
            # ç¡®ä¿è¯è¡¨å¤§å°åŒ¹é…
            self.config.vocab_size = self.tokenizer.vocab_size
        else:
            # ä½¿ç”¨é»˜è®¤é…ç½®
            self.config = MiniMindConfig(vocab_size=self.tokenizer.vocab_size)
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = MiniMindForCausalLM(self.config)
        
        # åŠ è½½æƒé‡
        checkpoint = torch.load(model_path, map_location=self.device)
        if isinstance(checkpoint, dict) and "model_state" in checkpoint:
            self.model.load_state_dict(checkpoint["model_state"], strict=False)
            print(f"ğŸ“Š å·²åŠ è½½æ£€æŸ¥ç‚¹ (epoch: {checkpoint.get('epoch', 'unknown')})")
        else:
            self.model.load_state_dict(checkpoint, strict=False)
        
        self.model.to(self.device)
        self.model.eval()
    
    def evaluate_dataset(
        self,
        data_path: str,
        num_workers: int = 2,
        show_samples: int = 5
    ) -> Dict[str, float]:
        """
        åœ¨æ•°æ®é›†ä¸Šè¯„ä¼°æ¨¡å‹
        
        Args:
            data_path: æ•°æ®é›†è·¯å¾„ï¼ˆJSONLæ ¼å¼ï¼‰
            num_workers: æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹æ•°
            show_samples: æ˜¾ç¤ºå¤šå°‘ä¸ªç”Ÿæˆæ ·æœ¬
            
        Returns:
            åŒ…å«å„é¡¹æŒ‡æ ‡çš„å­—å…¸
        """
        print(f"\n{'='*60}")
        print(f"å¼€å§‹è¯„ä¼°: {data_path}")
        print(f"{'='*60}\n")
        
        # åŠ è½½æ•°æ®é›†
        dataset = MinimindDataset(data_path, max_length=self.max_seq_len)
        dataloader = DataLoader(
            dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=True
        )
        
        print(f"ğŸ“Š æ•°æ®é›†å¤§å°: {len(dataset)} æ ·æœ¬")
        print(f"ğŸ“Š æ‰¹æ¬¡æ•°é‡: {len(dataloader)}\n")
        
        # è¯„ä¼°æŒ‡æ ‡
        total_loss = 0.0
        total_tokens = 0
        correct_predictions = 0
        
        # è¯„ä¼°å¾ªç¯
        with torch.no_grad():
            for batch_idx, (input_ids, labels, loss_mask) in enumerate(tqdm(dataloader, desc="è¯„ä¼°ä¸­")):
                input_ids = input_ids.to(self.device)
                labels = labels.to(self.device)
                loss_mask = loss_mask.to(self.device)
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(input_ids)
                logits = outputs.logits
                
                # è®¡ç®—æŸå¤±
                loss = self.criterion(
                    logits.reshape(-1, self.config.vocab_size),
                    labels.reshape(-1)
                ).reshape(labels.size())
                
                # åº”ç”¨maskï¼Œåªè®¡ç®—épaddingä½ç½®çš„æŸå¤±
                masked_loss = (loss * loss_mask).sum()
                num_tokens = loss_mask.sum()
                
                total_loss += masked_loss.item()
                total_tokens += num_tokens.item()
                
                # è®¡ç®—å‡†ç¡®ç‡
                predictions = logits.argmax(dim=-1)
                correct = ((predictions == labels) * loss_mask).sum()
                correct_predictions += correct.item()
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = self._calculate_metrics(total_loss, total_tokens, correct_predictions)
        
        # æ‰“å°ç»“æœ
        self._print_metrics(metrics)
        
        # ç”Ÿæˆæ ·æœ¬
        if show_samples > 0:
            self._generate_samples(dataset, show_samples)
        
        return metrics
    
    def _calculate_metrics(
        self,
        total_loss: float,
        total_tokens: int,
        correct_predictions: int
    ) -> Dict[str, float]:
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        avg_loss = total_loss / total_tokens if total_tokens > 0 else float('inf')
        perplexity = math.exp(avg_loss) if avg_loss < 20 else float('inf')  # é¿å…æº¢å‡º
        accuracy = correct_predictions / total_tokens if total_tokens > 0 else 0.0
        
        return {
            "loss": avg_loss,
            "perplexity": perplexity,
            "accuracy": accuracy,
            "total_tokens": total_tokens
        }
    
    def _print_metrics(self, metrics: Dict[str, float]):
        """æ‰“å°è¯„ä¼°æŒ‡æ ‡"""
        print(f"\n{'='*60}")
        print("è¯„ä¼°ç»“æœ:")
        print(f"{'='*60}")
        print(f"ğŸ“Š å¹³å‡æŸå¤± (Loss):        {metrics['loss']:.4f}")
        print(f"ğŸ“Š å›°æƒ‘åº¦ (Perplexity):     {metrics['perplexity']:.4f}")
        print(f"ğŸ“Š Tokenå‡†ç¡®ç‡ (Accuracy):  {metrics['accuracy']*100:.2f}%")
        print(f"ğŸ“Š è¯„ä¼°Tokenæ€»æ•°:           {metrics['total_tokens']:,}")
        print(f"{'='*60}\n")
    
    def _generate_samples(self, dataset: MinimindDataset, num_samples: int):
        """ç”Ÿæˆä¸€äº›æ–‡æœ¬æ ·æœ¬å±•ç¤ºæ¨¡å‹ç”Ÿæˆèƒ½åŠ›"""
        print(f"\n{'='*60}")
        print(f"ç”Ÿæˆæ ·æœ¬ (å…± {num_samples} ä¸ª):")
        print(f"{'='*60}\n")
        
        for i in range(min(num_samples, len(dataset))):
            # è·å–æ•°æ®é›†ä¸­çš„ä¸€ä¸ªæ ·æœ¬
            sample_data = dataset.data[i * (len(dataset) // num_samples)]
            prompt_text = sample_data.get("text", "")
            
            # æˆªå–å‰é¢ä¸€éƒ¨åˆ†ä½œä¸ºæç¤º
            if len(prompt_text) > 50:
                prompt = prompt_text[:50]
                expected = prompt_text[50:150]
            else:
                prompt = prompt_text[:len(prompt_text)//2]
                expected = prompt_text[len(prompt_text)//2:]
            
            # ç”Ÿæˆæ–‡æœ¬
            inputs = self.tokenizer(prompt, return_tensors="pt")
            input_ids = inputs["input_ids"].to(self.device)
            
            with torch.no_grad():
                output_ids = self.model.generate(
                    input_ids,
                    max_new_tokens=100,
                    temperature=0.8,
                    do_sample=True,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            generated = self.tokenizer.decode(output_ids[0].tolist())
            generated_continuation = generated[len(prompt):]
            
            print(f"æ ·æœ¬ {i+1}:")
            print(f"  æç¤º: {prompt}")
            print(f"  æœŸæœ›: {expected[:100]}...")
            print(f"  ç”Ÿæˆ: {generated_continuation[:100]}...")
            print()
    
    def evaluate_single_text(self, text: str) -> Dict[str, float]:
        """
        è¯„ä¼°å•ä¸ªæ–‡æœ¬æ ·æœ¬
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            åŒ…å«å„é¡¹æŒ‡æ ‡çš„å­—å…¸
        """
        # ç¼–ç æ–‡æœ¬
        inputs = self.tokenizer(text, return_tensors="pt", max_length=self.max_seq_len, truncation=True)
        input_ids = inputs["input_ids"].to(self.device)
        
        # å‡†å¤‡æ ‡ç­¾ï¼ˆå³ç§»ä¸€ä½ï¼‰
        labels = input_ids[:, 1:].contiguous()
        input_ids = input_ids[:, :-1].contiguous()
        
        with torch.no_grad():
            outputs = self.model(input_ids)
            logits = outputs.logits
            
            # è®¡ç®—æŸå¤±
            loss = self.criterion(
                logits.reshape(-1, self.config.vocab_size),
                labels.reshape(-1)
            )
            avg_loss = loss.mean().item()
            
            # è®¡ç®—å‡†ç¡®ç‡
            predictions = logits.argmax(dim=-1)
            correct = (predictions == labels).sum().item()
            accuracy = correct / labels.numel()
        
        perplexity = math.exp(avg_loss) if avg_loss < 20 else float('inf')
        
        return {
            "loss": avg_loss,
            "perplexity": perplexity,
            "accuracy": accuracy,
            "text_length": len(text)
        }


def main():
    parser = argparse.ArgumentParser(description="MiniMind æ¨¡å‹è¯„ä¼°è„šæœ¬")
    
    # æ¨¡å‹é…ç½®
    parser.add_argument(
        "--model_path",
        type=str,
        default="./output/minimind_model.pt",
        help="æ¨¡å‹æƒé‡è·¯å¾„"
    )
    parser.add_argument(
        "--tokenizer_path",
        type=str,
        default="./dataset/tokenizer",
        help="åˆ†è¯å™¨è·¯å¾„"
    )
    
    # æ•°æ®é…ç½®
    parser.add_argument(
        "--data_path",
        type=str,
        default="./dataset/pretrain.jsonl",
        help="è¯„ä¼°æ•°æ®é›†è·¯å¾„ï¼ˆJSONLæ ¼å¼ï¼‰"
    )
    parser.add_argument(
        "--max_seq_len",
        type=int,
        default=512,
        help="æœ€å¤§åºåˆ—é•¿åº¦"
    )
    
    # è¯„ä¼°é…ç½®
    parser.add_argument(
        "--batch_size",
        type=int,
        default=4,
        help="æ‰¹æ¬¡å¤§å°"
    )
    parser.add_argument(
        "--device",
        type=str,
        default="cuda",
        help="è®¡ç®—è®¾å¤‡ (cuda/cpu)"
    )
    parser.add_argument(
        "--num_workers",
        type=int,
        default=2,
        help="æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹æ•°"
    )
    parser.add_argument(
        "--show_samples",
        type=int,
        default=5,
        help="æ˜¾ç¤ºå¤šå°‘ä¸ªç”Ÿæˆæ ·æœ¬"
    )
    
    # è¾“å‡ºé…ç½®
    parser.add_argument(
        "--output_file",
        type=str,
        default=None,
        help="ä¿å­˜è¯„ä¼°ç»“æœçš„JSONæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰"
    )
    
    # å•æ–‡æœ¬è¯„ä¼°
    parser.add_argument(
        "--text",
        type=str,
        default=None,
        help="è¯„ä¼°å•ä¸ªæ–‡æœ¬ï¼ˆå¦‚æœæä¾›ï¼Œå°†ä¸ä½¿ç”¨æ•°æ®é›†ï¼‰"
    )
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = ModelEvaluator(
        model_path=args.model_path,
        tokenizer_path=args.tokenizer_path,
        device=args.device,
        batch_size=args.batch_size,
        max_seq_len=args.max_seq_len
    )
    
    # æ‰§è¡Œè¯„ä¼°
    if args.text:
        # è¯„ä¼°å•ä¸ªæ–‡æœ¬
        print(f"\nè¯„ä¼°å•ä¸ªæ–‡æœ¬:")
        print(f"æ–‡æœ¬: {args.text}\n")
        metrics = evaluator.evaluate_single_text(args.text)
        evaluator._print_metrics(metrics)
    else:
        # è¯„ä¼°æ•°æ®é›†
        metrics = evaluator.evaluate_dataset(
            data_path=args.data_path,
            num_workers=args.num_workers,
            show_samples=args.show_samples
        )
    
    # ä¿å­˜ç»“æœ
    if args.output_file:
        output_path = Path(args.output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(metrics, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {output_path}")


if __name__ == "__main__":
    main()
