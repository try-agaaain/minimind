"""
MiniMind æ•°æ®é›†å¤„ç† - ä½¿ç”¨ langchain-text-splitters å’Œ torch Dataset
"""
import json
import tempfile
from pathlib import Path
from typing import List, Optional

import torch
from torch.utils.data import Dataset
from tqdm import tqdm
from langchain_text_splitters import RecursiveCharacterTextSplitter

import os

from transformers import PreTrainedTokenizerFast, AutoTokenizer
from tokenizers import BertWordPieceTokenizer, Tokenizer

class MiniMindTokenizerFast(PreTrainedTokenizerFast):
    """
    MiniMind åˆ†è¯å™¨ - ç»§æ‰¿ PreTrainedTokenizerFast
    """
    
    # å‘Šè¯‰çˆ¶ç±»ï¼Œåº•å±‚ tokenizers åº“çš„æ–‡ä»¶å«ä»€ä¹ˆåå­—
    tokenizer_file = "tokenizer.json" 
    
    # æ¨¡åž‹è¾“å…¥åç§°
    model_input_names = ["input_ids", "attention_mask", "token_type_ids"]
    
    def __init__(
        self, 
        tokenizer_object: Optional[Tokenizer] = None, 
        unk_token="[UNK]", 
        pad_token="[PAD]", 
        cls_token="[CLS]", 
        sep_token="[SEP]", 
        mask_token="[MASK]", 
        **kwargs
    ):
        """
        åˆå§‹åŒ–æ–¹æ³•é…ç½®ç‰¹æ®Šæ ‡è®°å¹¶è°ƒç”¨çˆ¶ç±»çš„ __init__ã€‚
        
        Args:
            tokenizer_object: åº•å±‚çš„ tokenizers.Tokenizer å¯¹è±¡
            unk_token: æœªçŸ¥æ ‡è®°ï¼Œé»˜è®¤ "[UNK]"
            pad_token: å¡«å……æ ‡è®°ï¼Œé»˜è®¤ "[PAD]"
            cls_token: åˆ†ç±»æ ‡è®°ï¼Œé»˜è®¤ "[CLS]"
            sep_token: åˆ†éš”ç¬¦ï¼Œé»˜è®¤ "[SEP]"
            mask_token: æŽ©ç æ ‡è®°ï¼Œé»˜è®¤ "[MASK]"
            **kwargs: å…¶ä»–å‚æ•°ä¼ é€’ç»™çˆ¶ç±»
        """
        super().__init__(
            tokenizer_object=tokenizer_object,
            unk_token=unk_token,
            pad_token=pad_token,
            cls_token=cls_token,
            sep_token=sep_token,
            mask_token=mask_token,
            **kwargs,
        )

    @classmethod
    def from_pretrained(cls, model_id_or_path: str, **kwargs) -> "MiniMindTokenizerFast":
        """
        ä»Žé¢„è®­ç»ƒæ¨¡åž‹åŠ è½½ tokenizer
        
        Args:
            model_id_or_path: æ¨¡åž‹ ID æˆ–æœ¬åœ°è·¯å¾„
            **kwargs: å…¶ä»–å‚æ•°ä¼ é€’ç»™çˆ¶ç±»
        
        Returns:
            MiniMindTokenizerFast: åŠ è½½çš„ tokenizer å®žä¾‹
        """
        tokenizer = super().from_pretrained(model_id_or_path, **kwargs)
        print(f"âœ… å·²åŠ è½½ (è¯è¡¨å¤§å°: {tokenizer.vocab_size})")
        return tokenizer


# ============================================================
# æ­¥éª¤ 3: ä¾¿åˆ©å‡½æ•° - ç”¨äºŽè®­ç»ƒå’Œä¿å­˜æµç¨‹
# ============================================================

def train_and_save_tokenizer(
    files: List[str],
    save_path: str,
    vocab_size: int = 6400,
    min_frequency: int = 2
) -> MiniMindTokenizerFast:
    """
    è®­ç»ƒåˆ†è¯å™¨å¹¶ä¿å­˜ï¼ˆåŒ…æ‹¬ tokenizer.json å’Œ tokenizer_config.jsonï¼‰
    
    Args:
        files: è®­ç»ƒæ–‡æœ¬æ–‡ä»¶åˆ—è¡¨
        save_path: ä¿å­˜è·¯å¾„
        vocab_size: è¯è¡¨å¤§å°
        min_frequency: æœ€å°é¢‘çŽ‡
    
    Returns:
        MiniMindTokenizerFast: è®­ç»ƒåŽçš„ tokenizer
    """
    files = [f for f in files if os.path.exists(f)]
    if not files:
        raise ValueError("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è®­ç»ƒæ–‡ä»¶")

    # 1. åˆå§‹åŒ–åŽŸå§‹çš„ BertWordPiece Tokenizer
    tokenizer = BertWordPieceTokenizer(
        clean_text=True,
        handle_chinese_chars=True,
        lowercase=True,
    )

    special_tokens = ["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"]

    print(f"ðŸ“š è®­ç»ƒåˆ†è¯å™¨ ({len(files)} ä¸ªæ–‡ä»¶)...")
    
    # 2. è®­ç»ƒåˆ†è¯å™¨
    tokenizer.train(
        files=files,
        vocab_size=vocab_size,
        min_frequency=min_frequency,
        special_tokens=special_tokens,
    )
    
    print(f"âœ… è®­ç»ƒå®Œæˆ (è¯è¡¨å¤§å°: {tokenizer.get_vocab_size()})")

    # 3. ä¿å­˜åº•å±‚æ–‡ä»¶ (tokenizer.json)
    if save_path:
        Path(save_path).mkdir(parents=True, exist_ok=True)
        tokenizer.save(str(Path(save_path) / "tokenizer.json"), pretty=True)
        print(f"ðŸ’¾ å·²ä¿å­˜åˆ°: {save_path}/tokenizer.json")

    # 2. åˆ›å»º MiniMindTokenizerFast å®žä¾‹
    tokenizer = MiniMindTokenizerFast(tokenizer_object=tokenizer)
    
    # 3. ä¿å­˜é…ç½®æ–‡ä»¶ï¼ˆç”Ÿæˆ tokenizer_config.jsonï¼‰
    tokenizer.save_pretrained(save_path)
    print(f"âœ… åˆ†è¯å™¨å·²ä¿å­˜åˆ°: {save_path}")
    
    return tokenizer

class NovelDatasetPreparator:
    """å°è¯´æ•°æ®é›†å‡†å¤‡å™¨ - æ–‡æœ¬åˆ†å‰² -> tokenization -> JSONL"""
    
    def __init__(
        self,
        dataset_dir: str = "./dataset",
        chunk_size: int = 1024,
        chunk_overlap: int = 128,
        tokenizer_path: Optional[str] = None
    ):
        self.dataset_dir = Path(dataset_dir)
        self.output_path = Path(tokenizer_path)
        
        # åˆå§‹åŒ– tokenizer
        tokenizer_file = Path(tokenizer_path) if tokenizer_path else None
        if tokenizer_file and tokenizer_file.exists():
            self.tokenizer = MiniMindTokenizerFast.from_pretrained(str(tokenizer_file.parent))
        else:
            self.tokenizer = None
        
        # æ–‡æœ¬åˆ†å‰²å™¨
        self.splitter = RecursiveCharacterTextSplitter(
            separators=["\n\n", "\n", "ã€‚", "ï¼Œ", " ", ""],
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
        )
    
    def get_novel_files(self) -> list:
        """èŽ·å–æ‰€æœ‰å°è¯´æ–‡ä»¶"""
        return sorted(self.dataset_dir.rglob("*.txt"))
    
    def load_novel_text(self, file_path: Path) -> Optional[str]:
        """åŠ è½½å°è¯´æ–‡æœ¬ï¼Œæ”¯æŒå¤šç§ç¼–ç """
        for enc in ["utf-8", "gbk", "gb2312"]:
            try:
                return file_path.read_text(encoding=enc)
            except (UnicodeDecodeError, LookupError):
                continue
        print(f"âš ï¸  æ— æ³•è¯»å–æ–‡ä»¶: {file_path}")
        return None
    
    def prepare_dataset(self) -> None:
        """å‡†å¤‡æ•°æ®é›†å¹¶ä¿å­˜ä¸º JSONL"""
        novels = self.get_novel_files()
        print(f"ðŸ“š æ‰¾åˆ° {len(novels)} ä¸ªå°è¯´æ–‡ä»¶")
        
        self.output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # å¦‚æžœtokenizeræœªåˆå§‹åŒ–ï¼Œåˆ™å…ˆè®­ç»ƒ
        if self.tokenizer is None:
            temp_files = []
            for novel_path in novels:
                text = self.load_novel_text(novel_path)
                if text is None:
                    continue
                with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as f:
                    f.write('\n'.join(line.strip() for line in text.split('\n') if line.strip()))
                    temp_files.append(f.name)
            
            if temp_files:
                # ä½¿ç”¨æ–°çš„å‡½æ•°è¿›è¡Œè®­ç»ƒå’Œä¿å­˜
                self.tokenizer = train_and_save_tokenizer(
                    files=temp_files,
                    save_path=str(self.output_path.parent / "tokenizer"),
                    vocab_size=6400
                )
            
            for f in temp_files:
                import os
                os.unlink(f)
        
        # ç¼–ç å¹¶ä¿å­˜
        with open(self.output_path, 'w', encoding='utf-8') as out_f:
            for novel_path in tqdm(novels, desc="å¤„ç†"):
                text = self.load_novel_text(novel_path)
                if text is None:
                    continue
                text = '\n'.join(line.strip() for line in text.split('\n') if line.strip())
                
                for chunk in self.splitter.split_text(text):
                    out_f.write(json.dumps({
                        "text": chunk,
                        "token_ids": self.tokenizer.encode(chunk),
                        "source": novel_path.name,
                    }, ensure_ascii=False) + '\n')
        
        print(f"âœ… æ•°æ®é›†å·²ä¿å­˜: {self.output_path}")


class MinimindDataset(Dataset):
    """ä»Ž JSONL æ–‡ä»¶åŠ è½½å·² tokenized çš„æ•°æ®"""
    
    def __init__(self, jsonl_path: str, max_length: int = 512):
        self.data = []
        with open(jsonl_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    self.data.append(json.loads(line))
        
        self.max_length = max_length
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        token_ids = self.data[idx]["token_ids"]
        
        # æˆªæ–­æˆ–å¡«å……
        if len(token_ids) > self.max_length:
            token_ids = token_ids[:self.max_length]
        else:
            token_ids = token_ids + [0] * (self.max_length - len(token_ids))
        
        token_ids = torch.tensor(token_ids[:self.max_length], dtype=torch.long)
        return token_ids[:-1], token_ids[1:]

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser()
    parser.add_argument("--dataset_dir", default="./dataset")
    parser.add_argument("--chunk_size", type=int, default=1024)
    parser.add_argument("--chunk_overlap", type=int, default=128)
    parser.add_argument("--vocab_size", type=int, default=6400)
    parser.add_argument("--tokenizer_path", default=None)
    
    args = parser.parse_args()
    
    preparator = NovelDatasetPreparator(
        dataset_dir=args.dataset_dir,
        chunk_size=args.chunk_size,
        chunk_overlap=args.chunk_overlap,
        tokenizer_path=args.tokenizer_path
    )
    preparator.prepare_dataset()
