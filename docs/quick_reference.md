# MiniMind å¿«é€Ÿå‚è€ƒæ‰‹å†Œ

æœ¬æ–‡æ¡£æä¾› MiniMind å…³é”®æ¦‚å¿µå’Œä»£ç çš„å¿«é€ŸæŸ¥æ‰¾å‚è€ƒã€‚

## ç›®å½•

- [æ¨¡å‹é…ç½®é€ŸæŸ¥](#æ¨¡å‹é…ç½®é€ŸæŸ¥)
- [æ ¸å¿ƒç»„ä»¶é€ŸæŸ¥](#æ ¸å¿ƒç»„ä»¶é€ŸæŸ¥)
- [è®­ç»ƒå‚æ•°é€ŸæŸ¥](#è®­ç»ƒå‚æ•°é€ŸæŸ¥)
- [å¸¸ç”¨ä»£ç ç‰‡æ®µ](#å¸¸ç”¨ä»£ç ç‰‡æ®µ)

---

## æ¨¡å‹é…ç½®é€ŸæŸ¥

### é¢„è®¾é…ç½®

#### å°å‹æ¨¡å‹ (~50M å‚æ•°)
```python
config = MiniMindConfig(
    hidden_size=512,
    num_hidden_layers=8,
    num_attention_heads=8,
    num_key_value_heads=2,
    vocab_size=6400,
    max_position_embeddings=2048
)
```

#### ä¸­å‹æ¨¡å‹ (~200M å‚æ•°)
```python
config = MiniMindConfig(
    hidden_size=1024,
    num_hidden_layers=16,
    num_attention_heads=16,
    num_key_value_heads=4,
    vocab_size=32000,
    max_position_embeddings=4096
)
```

#### å¤§å‹æ¨¡å‹ (~1B å‚æ•°)
```python
config = MiniMindConfig(
    hidden_size=2048,
    num_hidden_layers=24,
    num_attention_heads=32,
    num_key_value_heads=8,
    vocab_size=32000,
    max_position_embeddings=8192
)
```

### MoE é…ç½®
```python
config = MiniMindConfig(
    # ... åŸºç¡€é…ç½® ...
    use_moe=True,
    num_experts_per_tok=2,      # æ¯ä¸ª token æ¿€æ´»çš„ä¸“å®¶æ•°
    n_routed_experts=8,          # è·¯ç”±ä¸“å®¶æ€»æ•°
    n_shared_experts=1,          # å…±äº«ä¸“å®¶æ•°
    aux_loss_alpha=0.1           # è¾…åŠ©æŸå¤±æƒé‡
)
```

### é•¿åºåˆ—é…ç½®ï¼ˆä½¿ç”¨ YaRNï¼‰
```python
config = MiniMindConfig(
    # ... åŸºç¡€é…ç½® ...
    max_position_embeddings=32768,  # 32K ä¸Šä¸‹æ–‡
    inference_rope_scaling=True,     # å¯ç”¨ YaRN
    rope_theta=1000000.0            # RoPE åŸºæ•°
)
```

---

## æ ¸å¿ƒç»„ä»¶é€ŸæŸ¥

### RMSNorm
**å…¬å¼**: `output = weight * (x / sqrt(mean(x^2) + eps))`

**ç”¨é€”**: 
- è¾“å…¥å½’ä¸€åŒ–ï¼ˆæ³¨æ„åŠ›å‰ï¼‰
- FFN å½’ä¸€åŒ–ï¼ˆFFN å‰ï¼‰
- æœ€ç»ˆå½’ä¸€åŒ–ï¼ˆè¾“å‡ºå‰ï¼‰

**å‚æ•°**: 
- `dim`: ç‰¹å¾ç»´åº¦
- `eps`: æ•°å€¼ç¨³å®šæ€§ï¼ˆé»˜è®¤ 1e-5ï¼‰

### RoPE (æ—‹è½¬ä½ç½®ç¼–ç )
**æ ¸å¿ƒæ€æƒ³**: é€šè¿‡æ—‹è½¬æ“ä½œæ³¨å…¥ä½ç½®ä¿¡æ¯

**é¢‘ç‡**: `Î¸_i = base^(-2i/d)`ï¼Œé»˜è®¤ `base = 1e6`

**åº”ç”¨ä½ç½®**: åªåº”ç”¨äº Q å’Œ Kï¼Œä¸åº”ç”¨äº V

**YaRN å¤–æ¨**: å½“åºåˆ—é•¿åº¦è¶…è¿‡è®­ç»ƒé•¿åº¦æ—¶è‡ªåŠ¨ç¼©æ”¾

### GQA (åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›)
**é…ç½®å…³ç³»**: 
```
n_heads = 8, n_kv_heads = 2
â†’ æ¯ä¸ª KV å¤´å¯¹åº” 4 ä¸ª Q å¤´
â†’ KV cache å‡å°‘ 75%
```

**Flash Attention**: PyTorch 2.0+ è‡ªåŠ¨å¯ç”¨ï¼ˆè®­ç»ƒæ›´å¿«ï¼‰

**KV Cache**: æ¨ç†æ—¶ç¼“å­˜å†å² K, Vï¼Œé¿å…é‡å¤è®¡ç®—

### SwiGLU FFN
**å…¬å¼**: `output = (silu(W_gate(x)) * W_up(x)) @ W_down`

**ä¸­é—´ç»´åº¦**: çº¦ä¸º `hidden_size * 2.67`ï¼ˆå‘ä¸Šå–æ•´åˆ° 64 çš„å€æ•°ï¼‰

**æ¿€æ´»å‡½æ•°**: SiLU (Swish) = `x * sigmoid(x)`

### MoE
**è·¯ç”±**: Top-K é€‰æ‹©ï¼Œæ¯ä¸ª token æ¿€æ´» K ä¸ªä¸“å®¶

**è´Ÿè½½å‡è¡¡**: è¾…åŠ©æŸå¤± `aux_loss = Î± * Î£(P_i * f_i)`

**å…±äº«ä¸“å®¶**: æ€»æ˜¯æ¿€æ´»ï¼Œå­¦ä¹ é€šç”¨ç‰¹å¾

---

## è®­ç»ƒå‚æ•°é€ŸæŸ¥

### æ¨èçš„è®­ç»ƒé…ç½®

#### å°æ¨¡å‹ï¼ˆGPU æ˜¾å­˜ < 8GBï¼‰
```bash
python train.py \
    --hidden_size 512 \
    --num_layers 8 \
    --num_heads 8 \
    --batch_size 2 \
    --learning_rate 1e-4 \
    --grad_clip 1.0
```

#### ä¸­ç­‰æ¨¡å‹ï¼ˆGPU æ˜¾å­˜ 16-24GBï¼‰
```bash
python train.py \
    --hidden_size 1024 \
    --num_layers 16 \
    --num_heads 16 \
    --batch_size 4 \
    --learning_rate 5e-5 \
    --grad_clip 1.0
```

#### ä½¿ç”¨ MoE
```bash
python train.py \
    --use_moe 1 \
    --hidden_size 1024 \
    --num_layers 16 \
    --batch_size 4 \
    --learning_rate 5e-5
```

### ä¼˜åŒ–å™¨å‚æ•°

| å‚æ•° | æ¨èå€¼ | è¯´æ˜ |
|------|--------|------|
| learning_rate | 1e-4 ~ 5e-4 | ä»é›¶è®­ç»ƒ |
| learning_rate | 1e-5 ~ 5e-5 | å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ |
| weight_decay | 0.01 ~ 0.1 | L2 æ­£åˆ™åŒ– |
| grad_clip | 1.0 | æ¢¯åº¦è£å‰ªé˜ˆå€¼ |
| betas | (0.9, 0.999) | AdamW é»˜è®¤å€¼ |

### å­¦ä¹ ç‡è°ƒåº¦

**é¢„çƒ­æ­¥æ•°**: æ€»æ­¥æ•°çš„ 1-10%
```python
warmup_steps = total_steps * 0.1
```

**ä½™å¼¦è¡°å‡**: é¢„çƒ­åä½¿ç”¨ä½™å¼¦å‡½æ•°è¡°å‡
```python
lr = base_lr * 0.5 * (1 + cos(Ï€ * progress))
```

---

## å¸¸ç”¨ä»£ç ç‰‡æ®µ

### åˆå§‹åŒ–æ¨¡å‹
```python
from minimind import MiniMindConfig, MiniMindForCausalLM

# åˆ›å»ºé…ç½®
config = MiniMindConfig(
    hidden_size=512,
    num_hidden_layers=8,
    num_attention_heads=8,
    num_key_value_heads=2
)

# åˆå§‹åŒ–æ¨¡å‹
model = MiniMindForCausalLM(config)
model = model.to(device)

# åŠ è½½é¢„è®­ç»ƒæƒé‡
state_dict = torch.load('model.pth', map_location=device)
model.load_state_dict(state_dict, strict=False)
```

### è®­ç»ƒå¾ªç¯ï¼ˆæ ‡å‡†ï¼‰
```python
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)
criterion = nn.CrossEntropyLoss()

model.train()
for epoch in range(epochs):
    for input_ids, labels in dataloader:
        input_ids, labels = input_ids.to(device), labels.to(device)
        
        # å‰å‘ä¼ æ’­
        outputs = model(input_ids)
        loss = criterion(outputs.logits.view(-1, vocab_size), labels.view(-1))
        
        # MoE è¾…åŠ©æŸå¤±
        if hasattr(outputs, 'aux_loss') and outputs.aux_loss:
            loss = loss + outputs.aux_loss
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
```

### è®­ç»ƒå¾ªç¯ï¼ˆæ··åˆç²¾åº¦ï¼‰
```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for epoch in range(epochs):
    for input_ids, labels in dataloader:
        optimizer.zero_grad()
        
        # æ··åˆç²¾åº¦å‰å‘
        with autocast():
            outputs = model(input_ids)
            loss = criterion(outputs.logits.view(-1, vocab_size), labels.view(-1))
            if hasattr(outputs, 'aux_loss') and outputs.aux_loss:
                loss = loss + outputs.aux_loss
        
        # ç¼©æ”¾åå‘ä¼ æ’­
        scaler.scale(loss).backward()
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        scaler.step(optimizer)
        scaler.update()
```

### æ¨ç†ç”Ÿæˆï¼ˆåŸºç¡€ï¼‰
```python
@torch.no_grad()
def generate(model, input_ids, max_new_tokens=50, temperature=1.0):
    model.eval()
    
    for _ in range(max_new_tokens):
        outputs = model(input_ids)
        logits = outputs.logits[:, -1, :] / temperature
        probs = F.softmax(logits, dim=-1)
        next_token = torch.multinomial(probs, num_samples=1)
        input_ids = torch.cat([input_ids, next_token], dim=1)
        
        if next_token.item() == eos_token_id:
            break
    
    return input_ids
```

### æ¨ç†ç”Ÿæˆï¼ˆä½¿ç”¨ KV Cacheï¼‰
```python
@torch.no_grad()
def generate_with_cache(model, input_ids, max_new_tokens=50):
    model.eval()
    past_key_values = None
    
    for _ in range(max_new_tokens):
        # ç¬¬ä¸€æ¬¡å¤„ç†å®Œæ•´è¾“å…¥ï¼Œä¹‹ååªå¤„ç†æ–° token
        model_inputs = input_ids if past_key_values is None else input_ids[:, -1:]
        
        outputs = model(model_inputs, past_key_values=past_key_values, use_cache=True)
        past_key_values = outputs.past_key_values
        
        next_token = torch.argmax(outputs.logits[:, -1, :], dim=-1, keepdim=True)
        input_ids = torch.cat([input_ids, next_token], dim=1)
        
        if next_token.item() == eos_token_id:
            break
    
    return input_ids
```

### Top-p (Nucleus) Sampling
```python
def top_p_sampling(logits, top_p=0.9, temperature=1.0):
    logits = logits / temperature
    sorted_logits, sorted_indices = torch.sort(logits, descending=True)
    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
    
    # ç§»é™¤ç´¯ç§¯æ¦‚ç‡è¶…è¿‡ top_p çš„ token
    sorted_indices_to_remove = cumulative_probs > top_p
    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
    sorted_indices_to_remove[..., 0] = 0
    
    indices_to_remove = sorted_indices[sorted_indices_to_remove]
    logits[indices_to_remove] = -float('Inf')
    
    probs = F.softmax(logits, dim=-1)
    next_token = torch.multinomial(probs, num_samples=1)
    return next_token
```

### æ•°æ®é›†ï¼ˆä½¿ç”¨çœŸå® Tokenizerï¼‰
```python
from transformers import AutoTokenizer

class TextDataset(Dataset):
    def __init__(self, texts, tokenizer, max_length=512):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.texts = texts
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = self.texts[idx]
        encoding = self.tokenizer(
            text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        input_ids = encoding['input_ids'].squeeze()
        labels = input_ids.clone()
        
        # å› æœè¯­è¨€å»ºæ¨¡ï¼šè¾“å…¥å’Œæ ‡ç­¾é”™ä½ä¸€ä½
        return input_ids[:-1], labels[1:]
```

### ä¿å­˜å’ŒåŠ è½½æ£€æŸ¥ç‚¹
```python
# ä¿å­˜
checkpoint = {
    'epoch': epoch,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'loss': loss,
    'config': config.__dict__
}
torch.save(checkpoint, 'checkpoint.pt')

# åŠ è½½
checkpoint = torch.load('checkpoint.pt', map_location=device)
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
start_epoch = checkpoint['epoch'] + 1
```

### è®¡ç®—æ¨¡å‹å‚æ•°é‡
```python
def count_parameters(model):
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"æ€»å‚æ•°: {total_params:,}")
    print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    print(f"å‚æ•°é‡: {total_params / 1e6:.2f}M")
    
    return total_params, trainable_params

count_parameters(model)
```

### æŸ¥çœ‹æ¨¡å‹ç»“æ„
```python
# æ‰“å°æ¨¡å‹æ¶æ„
print(model)

# ç»Ÿè®¡æ¯å±‚å‚æ•°
for name, param in model.named_parameters():
    print(f"{name}: {param.shape}, {param.numel():,} params")

# æŸ¥çœ‹é…ç½®
print(model.config)
```

---

## æ•…éšœæ’é™¤é€ŸæŸ¥

| é—®é¢˜ | å¯èƒ½åŸå›  | è§£å†³æ–¹æ¡ˆ |
|------|---------|---------|
| æŸå¤±å˜ä¸º NaN | æ¢¯åº¦çˆ†ç‚¸ã€å­¦ä¹ ç‡è¿‡å¤§ | é™ä½å­¦ä¹ ç‡ã€å¯ç”¨æ¢¯åº¦è£å‰ª |
| æŸå¤±ä¸ä¸‹é™ | å­¦ä¹ ç‡è¿‡å°ã€æ•°æ®é—®é¢˜ | å¢åŠ å­¦ä¹ ç‡ã€æ£€æŸ¥æ•°æ® |
| æ˜¾å­˜ä¸è¶³ | batch size å¤ªå¤§ | å‡å° batch sizeã€ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ |
| è®­ç»ƒå¤ªæ…¢ | æœªä½¿ç”¨ä¼˜åŒ– | å¯ç”¨æ··åˆç²¾åº¦ã€Flash Attention |
| è¿‡æ‹Ÿåˆ | æ¨¡å‹å¤ªå¤§ã€æ•°æ®å¤ªå°‘ | å¢åŠ  dropoutã€æƒé‡è¡°å‡ |
| ç”Ÿæˆé‡å¤ | Temperature å¤ªä½ | å¢åŠ  temperatureã€ä½¿ç”¨ top-p |
| ç”Ÿæˆæ··ä¹± | Temperature å¤ªé«˜ | é™ä½ temperature |

---

## æ€§èƒ½ä¼˜åŒ–æ¸…å•

- [ ] ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆ`autocast` + `GradScaler`ï¼‰
- [ ] å¯ç”¨ Flash Attentionï¼ˆPyTorch 2.0+ï¼‰
- [ ] ä½¿ç”¨ KV Cache åŠ é€Ÿæ¨ç†
- [ ] ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯æ¨¡æ‹Ÿå¤§ batch size
- [ ] ä½¿ç”¨ DataLoader çš„ `num_workers > 0`
- [ ] å›ºå®šåºåˆ—é•¿åº¦ï¼ˆé¿å…åŠ¨æ€å¡«å……ï¼‰
- [ ] ä½¿ç”¨ç¼–è¯‘æ¨¡å¼ï¼ˆPyTorch 2.0+ `torch.compile()`ï¼‰
- [ ] ä½¿ç”¨ GQA å‡å°‘ KV cache å¤§å°
- [ ] è€ƒè™‘ä½¿ç”¨ MoE æé«˜å‚æ•°æ•ˆç‡

---

**ğŸ’¡ æç¤º**: è¯¦ç»†çš„åŸç†å’Œå®ç°ç»†èŠ‚è¯·å‚è€ƒå®Œæ•´æ–‡æ¡£ï¼š
- [æ¨¡å‹æ¶æ„è¯¦è§£](./minimind_architecture.md)
- [è®­ç»ƒæŒ‡å—](./training_guide.md)
