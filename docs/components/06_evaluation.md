# 模型评估：从训练到部署的关键桥梁

在机器学习的完整工作流程中，模型评估是连接训练和部署的关键环节。一个训练好的语言模型究竟表现如何？它在真实场景中能否达到预期效果？这些问题的答案都需要通过系统化的评估来获得。对于大型语言模型（LLM）而言，评估不仅是衡量性能的工具，更是理解模型行为、发现潜在问题、指导模型改进的重要手段。

## 为什么需要模型评估

在深入评估方法之前，我们先思考一个根本问题：**为什么训练损失下降了，我们还需要单独的评估步骤？**

### 训练损失的局限性

训练过程中，我们通过最小化损失函数来优化模型参数。对于语言模型，这个损失通常是交叉熵损失——模型预测下一个 token 的概率分布与真实 token 的 one-hot 分布之间的差距。当训练损失从 8.0 降到 2.5，这确实说明模型在训练数据上的预测越来越准确。

但这只是故事的一部分。**训练损失只告诉我们模型在已见过的数据上表现如何**，它无法回答以下关键问题：

1. **泛化能力**：模型在未见过的数据上表现如何？训练损失可能很低，但如果模型只是记住了训练样本（过拟合），在新数据上可能表现糟糕。

2. **实用性能**：损失值 2.5 是好是坏？它对应的实际文本生成质量如何？两个模型的损失都是 2.5，但生成的文本质量可能大相径庭。

3. **不同维度的表现**：模型可能在某些类型的文本上表现很好（比如新闻），但在其他类型上很差（比如对话）。训练损失是所有样本的平均，掩盖了这种差异。

4. **与人类期望的对齐**：数学上最优的损失不一定对应人类认为"好"的输出。比如，模型可能学会了流畅的语法但生成了事实错误的内容。

这就是为什么我们需要独立的评估阶段——**使用训练时未见过的数据，通过多种指标，全面衡量模型的真实能力**。

### 评估数据集的重要性

评估数据集（validation set 或 test set）必须与训练集完全独立。**为什么这个独立性如此重要？**

想象一个考试场景：如果考试题目就是课后练习的原题，你可能得100分，但这不代表你真正掌握了知识——你可能只是记住了答案。同样，如果评估数据包含在训练数据中，模型可能只是记住了这些样本，而不是真正学会了语言的规律。

在实践中，我们通常会从完整数据集中划分出 10-20% 作为评估集。这部分数据在训练过程中被严格隔离，模型在整个训练期间都无法"看到"它们。只有在训练结束后，我们才用这些数据来评估模型的最终表现。

## 核心评估指标

对于语言模型，有几个核心指标被广泛使用。每个指标从不同角度反映模型的能力，让我们逐一深入理解。

### 1. 困惑度（Perplexity）：语言模型的标准尺度

困惑度是评估语言模型最经典、最重要的指标。它直观地回答了一个问题：**模型对下一个 token 有多"困惑"？**

#### 数学定义与直觉

给定一个测试序列 $x_1, x_2, ..., x_N$，语言模型为每个 token 分配一个条件概率 $P(x_i | x_1, ..., x_{i-1})$。困惑度定义为：

$$
\text{PPL} = \exp\left(-\frac{1}{N}\sum_{i=1}^{N} \log P(x_i | x_1, ..., x_{i-1})\right)
$$

这个公式看起来复杂，但我们可以逐步拆解理解它：

**第一步：对数概率的负和**。对于每个 token $x_i$，模型给出的概率是 $P(x_i | \text{context})$。我们对这个概率取对数，然后对所有 token 求和。为什么要取对数？因为概率的连乘会导致数值下溢（很多小于1的数相乘会趋近于0），对数将乘法变为加法，保持数值稳定性。

$$
\text{负对数似然} = -\sum_{i=1}^{N} \log P(x_i | x_1, ..., x_{i-1})
$$

这实际上就是交叉熵损失的总和。一个完美的模型对每个真实 token 都会给出概率 1（$\log 1 = 0$），所以负对数似然为 0。模型越不确定（给真实 token 的概率越低），这个值越大。

**第二步：平均化**。我们将总和除以 token 数量 $N$，得到每个 token 的平均负对数似然：

$$
\text{平均负对数似然} = -\frac{1}{N}\sum_{i=1}^{N} \log P(x_i | x_1, ..., x_{i-1})
$$

这正是我们通常所说的"平均交叉熵损失"或"bits per character"（如果是字符级模型）。

**第三步：指数化**。最后，我们对平均负对数似然取指数：

$$
\text{PPL} = \exp\left(\text{平均负对数似然}\right)
$$

**为什么要取指数？** 这一步将对数空间的值映射回原始的概率空间，使得困惑度有了直观的概率解释。

#### 困惑度的概率解释

困惑度可以理解为"等价分支因子"（equivalent branching factor）。如果模型对下一个 token 的选择完全均匀（每个可能的 token 概率相同），那么困惑度就等于词表大小。

举个例子：假设词表大小为 1000，一个完全随机的模型会给每个 token 分配 1/1000 的概率。此时：

$$
\text{PPL} = \exp\left(-\frac{1}{N}\sum_{i=1}^{N} \log \frac{1}{1000}\right) = \exp(\log 1000) = 1000
$$

现在假设一个好的语言模型，它能够利用上下文信息，将候选 token 缩小到大约 10 个等可能的选项（每个概率 1/10）。此时：

$$
\text{PPL} = \exp\left(-\frac{1}{N}\sum_{i=1}^{N} \log \frac{1}{10}\right) = \exp(\log 10) = 10
$$

这个解释揭示了困惑度的核心含义：**困惑度为 10 意味着模型在预测下一个 token 时，平均需要在 10 个等可能的选项中选择**。困惑度越低，模型对下一个 token 的预测越确定。

#### 困惑度与交叉熵的关系

困惑度和交叉熵损失之间有简单的数学关系：

$$
\text{PPL} = \exp(\text{Loss}) = e^{\text{Loss}}
$$

其中 Loss 是平均交叉熵损失。这意味着：
- 损失为 2.3 → 困惑度为 $e^{2.3} \approx 10$
- 损失为 4.6 → 困惑度为 $e^{4.6} \approx 100$
- 损失为 6.9 → 困惑度为 $e^{6.9} \approx 1000$

**为什么报告困惑度而不是损失？** 主要原因是困惑度有更直观的解释（"平均需要从多少个选项中选择"），而损失值的绝对大小不够直观。不过在实践中，两者都会被关注——损失值在训练时更常用（因为优化的直接目标），困惑度在评估时更常用（因为更易理解）。

#### 好的困惑度是多少？

困惑度的"好坏"标准取决于任务和数据集：

- **字符级模型**（词表 ~100）：困惑度通常在 1.5-3 之间
- **词级模型**（词表 ~10,000）：困惑度通常在 50-200 之间
- **子词级模型**（词表 ~50,000）：困惑度通常在 20-100 之间

对于 MiniMind 这样的小型模型（词表 6400），一个合理的困惑度可能在 30-80 之间，具体取决于训练数据的质量和模型的训练程度。

**重要提示**：不同词表大小的困惑度不能直接比较。词表越大，随机基线的困惑度越高，所以达到的绝对困惑度值也可能更高。比较困惑度时，应该在相同设置（相同词表、相同数据集）下进行。

### 2. Token 准确率：直接的预测质量

虽然困惑度是最常用的指标，但它基于概率分布，有时不够直观。Token 准确率提供了一个更简单直接的视角：**模型预测正确的 token 比例是多少？**

#### 定义与计算

对于每个位置 $i$，模型输出一个概率分布，我们选择概率最高的 token 作为预测：

$$
\hat{x}_i = \arg\max_{x} P(x | x_1, ..., x_{i-1})
$$

然后统计预测正确的比例：

$$
\text{Accuracy} = \frac{1}{N}\sum_{i=1}^{N} \mathbb{1}[\hat{x}_i = x_i]
$$

其中 $\mathbb{1}[\cdot]$ 是指示函数，条件为真时值为 1，否则为 0。

#### 准确率的意义与局限

准确率的优点是非常直观——50% 准确率意味着模型预测对了一半的 token。但它也有明显的局限：

**局限1：忽略了"差距"的大小**。假设真实 token 是"学习"，模型有两种错误预测：
- 预测A："学校"（概率 0.45，真实 token "学习" 概率 0.40）
- 预测B："汽车"（概率 0.80，真实 token "学习" 概率 0.01）

准确率将这两种错误同等对待，但显然预测A更接近正确——模型几乎猜对了。困惑度能够区分这种差异（预测A的损失更小），而准确率不能。

**局限2：对于高度开放的任务不够敏感**。在某些上下文中，多个 token 都是合理的延续：

> 上下文："今天天气真"
> 合理延续："好" / "热" / "冷" / "差"

如果真实 token 是"好"但模型预测"热"，准确率计为 0，但这个预测其实也是合理的。困惑度会给"热"一个较高的概率（即使不是最高），从而不会过度惩罚这种"合理但不完全一致"的预测。

**局限3：不能反映生成质量**。两个模型可能有相同的 token 准确率，但生成的完整文本质量差异很大。一个模型可能倾向于选择安全但平淡的词，另一个模型可能更有创造性但偶尔犯错。

尽管有这些局限，准确率仍然是一个有用的补充指标。它在以下场景特别有价值：
- 快速对比不同模型的粗略表现
- 检测模型是否学到了基本的语言模式（随机基线的准确率很低）
- 分析特定类型错误的分布（如哪些词性的 token 更容易预测错）

#### 准确率的典型范围

对于语言建模任务，token 准确率通常不会很高：

- **随机基线**（词表 6400）：准确率约 0.016% (1/6400)
- **简单 n-gram 模型**：准确率约 5-15%
- **小型神经语言模型**（如 MiniMind）：准确率约 20-40%
- **大型预训练模型**（如 GPT-3）：准确率约 50-70%

注意即使是最先进的模型，准确率也很难超过 70%。这是因为语言的复杂性和歧义性——很多情况下，多个 token 都是合理的延续。这也再次说明，**单纯追求准确率不是语言模型的目标，生成高质量、连贯、有意义的文本才是**。

### 3. 损失函数：优化的直接目标

交叉熵损失（Cross-Entropy Loss）是训练语言模型时优化的目标函数，在评估时它同样是重要的指标。

#### 数学形式

对于分类问题，交叉熵损失衡量模型预测的概率分布 $P$ 与真实分布 $Q$ 之间的差异：

$$
\text{CE}(Q, P) = -\sum_{x} Q(x) \log P(x)
$$

在语言建模中，真实分布 $Q$ 是一个 one-hot 分布（真实 token 的位置为 1，其他为 0）。假设真实 token 是 $x^*$，则：

$$
\text{CE} = -\log P(x^* | \text{context})
$$

对整个序列，我们计算平均损失：

$$
\text{Loss} = -\frac{1}{N}\sum_{i=1}^{N} \log P(x_i | x_1, ..., x_{i-1})
$$

#### 损失值的解释

损失值是一个非负数，越小越好：

- **损失为 0**：模型对所有真实 token 都给出了概率 1（$\log 1 = 0$），即完美预测。在实践中几乎不可能达到。

- **损失为 $\log V$**（$V$ 是词表大小）：对应完全随机的预测（每个 token 概率 $1/V$）。对于词表 6400 的模型，随机基线的损失约为 $\log 6400 \approx 8.76$。

- **损失在 2-4 之间**：表示模型已经学到了一定的语言模式，能够基于上下文做出合理的预测。

- **损失低于 2**：通常表示模型性能很好，或者可能存在数据泄漏（评估数据包含在训练数据中）。

#### 损失与困惑度、准确率的关系

这三个指标之间有紧密的数学联系：

1. **损失 ↔ 困惑度**：$\text{PPL} = e^{\text{Loss}}$（严格的数学关系）
2. **损失 ↔ 准确率**：负相关，但不是简单的函数关系

一个有趣的观察：**损失下降不一定总是伴随准确率上升**。考虑这个例子：

> 上下文："今天天气真"
> 真实 token："好"

**模型A** 的预测分布：["好": 0.40, "热": 0.30, "冷": 0.20, "差": 0.10]
- 损失：$-\log 0.40 = 0.92$
- 准确率：0（预测"好"但概率不是最高... 等等，这里"好"概率最高，准确率应该是1）

让我修正这个例子：

**模型A** 的预测分布：["好": 0.40, "热": 0.35, "冷": 0.15, "差": 0.10]
- 损失：$-\log 0.40 = 0.92$
- 准确率：1（正确预测）

**模型B** 的预测分布：["好": 0.39, "热": 0.40, "冷": 0.11, "差": 0.10]
- 损失：$-\log 0.39 = 0.94$
- 准确率：0（错误预测为"热"）

这个例子说明：损失从 0.92 提升到 0.94（略微变差），但准确率从 1 降到 0（大幅下降）。这是因为准确率只关心最高概率的 token 是否正确，而损失关心真实 token 的概率绝对值。

在实践中，我们通常会同时关注这两个指标——损失反映模型的整体不确定性，准确率反映"赢家通吃"式的预测正确性。

### 4. 生成样本：定性评估的窗口

数值指标（困惑度、准确率、损失）虽然客观可比，但它们无法直接展示模型生成文本的实际质量。**生成样本评估**填补了这个空白——通过让模型续写一些提示文本，我们可以直观地感受模型的语言能力。

#### 为什么需要生成样本

想象两个模型，都有相同的困惑度 50，但：
- **模型A** 生成的文本流畅、连贯，但偶尔会出现事实错误
- **模型B** 生成的文本语法完美，但内容空洞、重复

困惑度无法区分这两种模型，因为它只衡量概率分布的质量，不衡量生成文本的语义连贯性、信息量或创造性。通过实际查看生成的文本，我们能发现这些数值指标无法捕捉的问题：

1. **重复问题**：模型是否倾向于生成重复的短语或句子？
2. **连贯性**：生成的多个句子之间是否有逻辑联系？
3. **多样性**：相同提示下，模型能否生成风格和内容不同的延续？
4. **事实性**：生成的内容是否符合常识和事实？
5. **风格保持**：模型是否能保持输入提示的文体风格？

#### 生成策略：贪婪 vs 采样

在生成文本时，我们需要从模型的概率分布中选择下一个 token。有几种常用策略：

**贪婪解码（Greedy Decoding）**：每次选择概率最高的 token

$$
x_i = \arg\max_{x} P(x | x_1, ..., x_{i-1})
$$

优点是确定性（相同输入总是产生相同输出）、速度快。缺点是可能导致重复、缺乏多样性，且一旦选择错误就无法纠正。

**随机采样（Sampling）**：按照概率分布随机采样

$$
x_i \sim P(\cdot | x_1, ..., x_{i-1})
$$

这增加了多样性，但可能选择低概率的不合理 token。为了控制随机性，通常会引入温度参数：

$$
P_T(x_i = k) = \frac{\exp(z_k / T)}{\sum_j \exp(z_j / T)}
$$

其中 $z_k$ 是模型输出的 logits（未归一化的分数），$T$ 是温度：
- $T = 1$：使用原始分布
- $T < 1$（如 0.5）：分布更尖锐，倾向于高概率选项（接近贪婪）
- $T > 1$（如 1.5）：分布更平滑，增加多样性（但可能不连贯）

**Top-k 采样**：只从概率最高的 $k$ 个 token 中采样

这避免了从长尾低概率 token 中采样不合理的词，同时保持一定的多样性。

**Top-p (核采样)**：从累积概率达到 $p$ 的最小 token 集合中采样

这是一种动态的选择策略——在确定性高的位置（一个 token 概率很高）采样范围小，在不确定位置采样范围大。

在评估中，我们通常会展示多种策略的结果：
- 贪婪解码的结果展示"模型最有信心的输出"
- 中等温度采样（$T=0.8$）展示"自然且有一定创造性的输出"
- 高温度采样（$T=1.2$）展示"模型的多样性边界"

#### 样本数量的选择

展示多少生成样本合适？这取决于评估目的：

- **快速检查**：3-5 个样本，快速感受模型的基本生成能力
- **全面评估**：10-20 个样本，覆盖不同类型的提示（短/长、事实/创意、不同主题）
- **细致分析**：50+ 个样本，用于深入的错误分析和模式识别

在 MiniMind 的评估脚本中，默认展示 5 个样本，这是一个平衡点——既不会输出过多信息（难以阅读），也足够展示模型的基本特征。

## 评估流程的设计

理解了各个指标后，我们来看如何设计一个完整的评估流程。

### 数据加载与批处理

评估通常在大量数据上进行（数千到数万个样本），直接将所有数据加载到内存不现实。我们需要使用批处理（batching）：

1. **数据集封装**：使用 `torch.utils.data.Dataset` 包装评估数据
2. **数据加载器**：使用 `DataLoader` 实现高效的批次迭代
3. **批次大小选择**：在内存允许的范围内尽可能大，以加速评估

在 MiniMind 的实现中，评估数据来自 JSONL 文件，每行是一个已分词的样本。`MinimindDataset` 类负责加载和预处理：

```python
class MinimindDataset(Dataset):
    def __getitem__(self, idx):
        token_ids = self.data[idx]["token_ids"]
        # 截断或填充到固定长度
        token_ids = token_ids[:self.max_length]
        # 返回 (input, label, mask)
        return token_ids[:-1], token_ids[1:], loss_mask
```

这里的关键设计是**输入-标签对的构造**：
- **输入**：`token_ids[:-1]`（去掉最后一个 token）
- **标签**：`token_ids[1:]`（去掉第一个 token）

这实现了"预测下一个 token"的任务——输入第 $i$ 个 token，预测第 $i+1$ 个 token。

### 评估循环

评估循环的基本结构是：

```python
model.eval()  # 切换到评估模式（关闭 dropout 等）
with torch.no_grad():  # 不计算梯度（节省内存和计算）
    for batch in dataloader:
        input_ids, labels, loss_mask = batch
        outputs = model(input_ids)
        # 计算损失、准确率等
```

**为什么需要 `model.eval()` 和 `torch.no_grad()`？**

- `model.eval()`：告诉模型进入评估模式。这会改变某些层的行为（如 BatchNorm 使用累积统计量而非批次统计量，Dropout 被关闭）。评估时我们想要确定性的行为，不希望有随机性。

- `torch.no_grad()`：告诉 PyTorch 不要构建计算图和存储梯度。在训练时，我们需要梯度来更新参数；但在评估时，我们只需要前向传播的结果。不计算梯度可以显著减少内存占用（梯度占用与激活值相当的内存）和计算时间。

### 损失掩码（Loss Mask）

在语言建模中，我们经常需要处理不同长度的序列。为了批处理，我们将它们填充（padding）到相同长度。但填充的 token 是人为添加的，不应该被计算损失。这就是 **loss mask** 的作用：

```python
loss = criterion(logits.view(-1, vocab_size), labels.view(-1))
loss = loss.view(labels.size())  # [batch_size, seq_len]
masked_loss = (loss * loss_mask).sum() / loss_mask.sum()
```

`loss_mask` 是一个 0/1 矩阵，真实 token 位置为 1，填充位置为 0。通过元素乘法，我们将填充位置的损失清零，然后除以有效 token 数量得到平均值。

**为什么不能直接忽略填充位置？** 因为 PyTorch 的损失函数默认对所有位置求平均。如果我们直接用包含填充的标签计算损失，填充 token（通常编码为 0）的损失会被包含进来，污染了真实的损失值。

### 指标聚合

在每个批次上，我们计算损失、准确率等，但最终需要报告整个数据集上的指标。简单的平均可能不准确：

**错误做法**：
```python
avg_loss = sum(batch_losses) / len(batch_losses)
```

这是错的，因为不同批次可能有不同数量的有效 token（由于序列长度不同和填充）。

**正确做法**：
```python
total_loss = sum(batch_loss * batch_token_count)
total_tokens = sum(batch_token_count)
avg_loss = total_loss / total_tokens
```

我们按 token 数量加权聚合，确保每个 token 对最终指标的贡献相同。

## 实现细节

让我们深入 MiniMind 评估脚本的具体实现，理解每个设计选择的原因。

### 模型加载

```python
def _load_model(self, model_path: str):
    # 尝试加载配置
    config_path = Path(model_path).parent / "config.json"
    if config_path.exists():
        self.config = MiniMindConfig.from_pretrained(str(config_path.parent))
        self.config.vocab_size = self.tokenizer.vocab_size
    else:
        self.config = MiniMindConfig(vocab_size=self.tokenizer.vocab_size)
```

这里的设计考虑了灵活性：
1. **优先使用保存的配置**：如果训练时保存了 `config.json`，加载它以确保架构一致
2. **匹配词表大小**：即使加载了配置，也要确保词表大小与 tokenizer 一致（防止配置文件过期）
3. **回退到默认配置**：如果没有配置文件，使用默认参数创建模型

**为什么需要同时加载配置和权重？** 因为模型的架构（层数、隐藏维度等）必须与保存的权重匹配。如果我们用错误的架构（比如 8 层模型去加载 12 层模型的权重），会导致形状不匹配的错误。

### 检查点格式处理

```python
checkpoint = torch.load(model_path, map_location=self.device)
if isinstance(checkpoint, dict) and "model_state" in checkpoint:
    self.model.load_state_dict(checkpoint["model_state"], strict=False)
    self.epoch = checkpoint.get("epoch", 0)
else:
    self.model.load_state_dict(checkpoint, strict=False)
```

这段代码处理两种常见的检查点格式：
1. **完整检查点**：包含 `model_state`、`optimizer_state`、`epoch` 等信息的字典
2. **纯权重**：只包含模型参数的 state dict

**为什么使用 `strict=False`？** 这允许部分加载——如果检查点中有额外的键（比如训练时有但评估时不需要的 buffer），或者缺少某些键（比如新加的层），不会报错。这在模型演化过程中很有用，但要小心确保关键参数都被正确加载。

### 设备管理

```python
self.device = torch.device(device if torch.cuda.is_available() else "cpu")
torch.cuda.set_device(self.device)
```

**为什么要检查 CUDA 可用性？** 用户可能在没有 GPU 的机器上运行评估（虽然会很慢）。自动回退到 CPU 提供了更好的可用性。

**为什么要 `set_device`？** 在多 GPU 系统中，这确保后续的张量创建默认在指定设备上，避免设备不匹配的错误。

### 困惑度计算的数值稳定性

```python
perplexity = math.exp(avg_loss) if avg_loss < 20 else float('inf')
```

**为什么对 `avg_loss` 有上限检查？** 因为 $e^{20} \approx 485,000,000$，而 $e^{100}$ 会导致浮点数溢出（超过 `float` 能表示的最大值 $\approx 10^{308}$）。当损失很大时（模型很差或训练刚开始），困惑度会爆炸。设置上限避免了数值错误，同时 `inf` 清楚地表示"模型表现极差"。

### 生成样本的策略

```python
output_ids = self.model.generate(
    input_ids,
    max_new_tokens=100,
    temperature=0.8,
    do_sample=True,
    pad_token_id=self.tokenizer.pad_token_id,
    eos_token_id=self.tokenizer.eos_token_id
)
```

参数选择的考量：
- **max_new_tokens=100**：足够展示模型的续写能力，但不会太长（避免评估时间过长）
- **temperature=0.8**：略低于 1.0，平衡流畅性和多样性
- **do_sample=True**：使用采样而非贪婪解码，展示模型的创造性
- **pad_token_id 和 eos_token_id**：确保生成过程正确处理特殊 token

## 评估结果的解读

当我们运行评估脚本，会得到类似这样的输出：

```
==================================================
评估结果:
==================================================
📊 平均损失 (Loss):        3.2456
📊 困惑度 (Perplexity):     25.67
📊 Token准确率 (Accuracy):  28.5%
📊 评估Token总数:           1,245,678
==================================================
```

如何解读这些数字？

### 损失值 3.24

这个损失表示模型对每个 token 平均的负对数似然。我们可以这样理解它：
- $3.24 = -\log P(\text{真实token})$
- 因此 $P(\text{真实token}) = e^{-3.24} \approx 0.039$

也就是说，模型平均给真实 token 分配了约 3.9% 的概率。这看起来很低，但考虑到词表有 6400 个 token，随机基线只有 0.016% 的概率，3.9% 已经是巨大的进步了。

### 困惑度 25.67

困惑度告诉我们，模型在预测下一个 token 时，平均在约 26 个选项中纠结。对比：
- 随机模型：困惑度 6400（完全不确定）
- 当前模型：困惑度 26（将不确定性降低了 99.6%）

这表明模型已经学会了大量语言知识，能够根据上下文大幅缩小候选范围。

### 准确率 28.5%

模型猜对了约 28.5% 的 token。这意味着：
- 每 4 个 token 中，有大约 1 个被正确预测
- 对于简单、确定性高的 token（如标点、常用词），准确率可能很高（>60%）
- 对于困难、歧义性大的 token（如专有名词、罕见词），准确率可能很低（<10%）

综合来看，这些指标表明模型已经学到了基本的语言模式，但还有很大的提升空间。

## 评估的最佳实践

基于经验，以下是一些评估时的建议：

### 1. 使用独立的评估集

**永远不要在训练集上评估**。即使是"验证集"也应该在训练过程中保持独立（虽然可以用于调参）。最终报告性能时，应该使用完全未见过的测试集。

### 2. 固定随机种子

评估时设置随机种子，确保结果可重复：

```python
torch.manual_seed(42)
np.random.seed(42)
```

这对于生成样本尤其重要——相同的种子应该产生相同的生成结果。

### 3. 多次评估取平均

对于有随机性的评估（如采样生成），运行多次并报告平均值和标准差：

```
困惑度: 25.67 ± 0.43 (3次运行)
```

这表明结果的稳定性。

### 4. 记录所有超参数

保存评估时使用的所有配置：模型路径、数据集路径、批次大小、设备等。这确保评估可以被精确复现。

### 5. 对比基线

始终包含基线对比：
- 随机基线（词表大小的困惑度）
- 简单基线（如 n-gram 模型）
- 之前版本的模型

这提供了性能改进的上下文。

## 总结

模型评估是机器学习流程中不可或缺的环节。通过困惑度、准确率、损失等量化指标，结合生成样本的定性分析，我们能够全面了解模型的能力和局限。

MiniMind 的评估脚本实现了这些核心功能：
- 在独立数据集上计算困惑度、准确率、损失
- 展示生成样本以直观感受模型质量
- 支持单文本和批量数据集评估
- 提供灵活的命令行接口

理解评估指标的数学原理和实际意义，不仅帮助我们正确解读模型性能，还能指导模型改进的方向。当困惑度下降停滞时，可能需要更多数据或更大的模型；当准确率很低但困惑度不错时，可能需要调整生成策略；当生成样本重复时，可能需要增加多样性惩罚。

评估不是终点，而是迭代改进的起点。通过持续的评估-分析-改进循环，我们能够构建出更强大、更可靠的语言模型。
