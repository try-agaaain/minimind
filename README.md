# MiniMind Fine-Tuning åˆ†æ”¯

è¿™ä¸ªåˆ†æ”¯ä¸“é—¨ç”¨äº MiniMind æ¨¡å‹çš„å¾®è°ƒè®­ç»ƒã€‚

## ğŸ“š æŠ€æœ¯æ–‡æ¡£

**æ·±å…¥ç†è§£ MiniMind çš„æ¶æ„å’Œè®­ç»ƒç»†èŠ‚ï¼Œè¯·æŸ¥çœ‹å®Œæ•´æŠ€æœ¯æ–‡æ¡£ï¼š**

- ğŸ“– **[æ–‡æ¡£æ€»è§ˆ](./docs/README.md)** - æ–‡æ¡£å¯¼èˆªå’Œé˜…è¯»å»ºè®®
- ğŸ—ï¸ **[æ¨¡å‹æ¶æ„è¯¦è§£](./docs/minimind_architecture.md)** - æ·±å…¥è®²è§£ç½‘ç»œç»“æ„å’Œæ ¸å¿ƒæŠ€æœ¯
  - RMSNorm å½’ä¸€åŒ–
  - æ—‹è½¬ä½ç½®ç¼–ç  (RoPE)
  - åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ› (GQA)
  - å‰é¦ˆç¥ç»ç½‘ç»œ (SwiGLU)
  - æ··åˆä¸“å®¶æ¨¡å‹ (MoE)
- ğŸ¯ **[è®­ç»ƒæŒ‡å—](./docs/training_guide.md)** - å…¨é¢çš„è®­ç»ƒæµç¨‹å’ŒæŠ€å·§
  - æ•°æ®å¤„ç†ä¸ Tokenization
  - ä¼˜åŒ–å™¨ä¸å­¦ä¹ ç‡è°ƒåº¦
  - è®­ç»ƒæŠ€å·§ï¼ˆæ¢¯åº¦è£å‰ªã€æ··åˆç²¾åº¦ç­‰ï¼‰
  - æŸå¤±å‡½æ•°ä¸æ¨ç†ç”Ÿæˆ
  - ç›‘æ§ä¸è°ƒè¯•

> ğŸ’¡ **æç¤º**ï¼šæ–‡æ¡£é‡‡ç”¨ step-by-step çš„è®²è§£æ–¹å¼ï¼Œä»æŠ€æœ¯èƒŒæ™¯åˆ°å®ç°ç»†èŠ‚ï¼Œé€‚åˆæœ‰æ·±åº¦å­¦ä¹ åŸºç¡€ä½†å¯¹ LLM ä¸å¤Ÿäº†è§£çš„å­¦ä¹ è€…ã€‚

## æ–‡ä»¶è¯´æ˜

- `minimind.py`: MiniMind æ¨¡å‹å®šä¹‰æ–‡ä»¶ (ä» master åˆ†æ”¯çš„ model/model_minimind.py å¤ç”¨)
- `train.py`: ç®€æ´çš„æ¨¡å‹è®­ç»ƒè„šæœ¬
- `docs/`: è¯¦ç»†çš„æŠ€æœ¯æ–‡æ¡£

## å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
pip install torch transformers
```

### 2. è¿è¡Œè®­ç»ƒ

æœ€ç®€å•çš„è®­ç»ƒæ–¹å¼:
```bash
python train.py
```

### 3. è‡ªå®šä¹‰è®­ç»ƒå‚æ•°

```bash
python train.py \
    --hidden_size 512 \
    --num_layers 8 \
    --num_heads 8 \
    --epochs 5 \
    --batch_size 4 \
    --learning_rate 1e-4 \
    --output_dir ./output
```

### 4. ä½¿ç”¨é¢„è®­ç»ƒæƒé‡

```bash
python train.py --pretrained_path /path/to/pretrained/model.pth
```

## å‚æ•°è¯´æ˜

### æ¨¡å‹é…ç½®
- `--hidden_size`: éšè—å±‚ç»´åº¦ (é»˜è®¤: 512)
- `--num_layers`: éšè—å±‚æ•°é‡ (é»˜è®¤: 8)
- `--num_heads`: æ³¨æ„åŠ›å¤´æ•°é‡ (é»˜è®¤: 8)
- `--vocab_size`: è¯è¡¨å¤§å° (é»˜è®¤: 6400)
- `--max_seq_len`: æœ€å¤§åºåˆ—é•¿åº¦ (é»˜è®¤: 512)
- `--dropout`: Dropoutç‡ (é»˜è®¤: 0.1)
- `--use_moe`: æ˜¯å¦ä½¿ç”¨MoEæ¶æ„ (é»˜è®¤: 0)

### è®­ç»ƒé…ç½®
- `--epochs`: è®­ç»ƒè½®æ•° (é»˜è®¤: 3)
- `--batch_size`: æ‰¹æ¬¡å¤§å° (é»˜è®¤: 4)
- `--learning_rate`: å­¦ä¹ ç‡ (é»˜è®¤: 1e-4)
- `--weight_decay`: æƒé‡è¡°å‡ (é»˜è®¤: 0.01)
- `--grad_clip`: æ¢¯åº¦è£å‰ª (é»˜è®¤: 1.0)

### å…¶ä»–é…ç½®
- `--device`: è®­ç»ƒè®¾å¤‡ (é»˜è®¤: cuda:0)
- `--output_dir`: æ¨¡å‹ä¿å­˜ç›®å½• (é»˜è®¤: ./output)
- `--pretrained_path`: é¢„è®­ç»ƒæƒé‡è·¯å¾„ (é»˜è®¤: "")
- `--log_interval`: æ—¥å¿—æ‰“å°é—´éš” (é»˜è®¤: 10)
- `--save_interval`: æ¨¡å‹ä¿å­˜é—´éš” (é»˜è®¤: 1 epoch)

## æ³¨æ„äº‹é¡¹

1. å½“å‰çš„è®­ç»ƒè„šæœ¬ä½¿ç”¨ç®€å•çš„å­—ç¬¦çº§tokenizationä½œä¸ºç¤ºä¾‹
2. å®é™…ä½¿ç”¨æ—¶ï¼Œå»ºè®®ä½¿ç”¨ä¸“ä¸šçš„tokenizer
3. å¯ä»¥æ ¹æ®éœ€è¦æ‰©å±•æ•°æ®åŠ è½½éƒ¨åˆ†ï¼Œæ”¯æŒä»æ–‡ä»¶è¯»å–æ•°æ®
4. è®­ç»ƒçš„æ¨¡å‹ä¼šä¿å­˜åœ¨ `output_dir` æŒ‡å®šçš„ç›®å½•ä¸­

## ç¤ºä¾‹è¾“å‡º

è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šçœ‹åˆ°ç±»ä¼¼å¦‚ä¸‹çš„è¾“å‡º:

```
==================================================
MiniMind è®­ç»ƒé…ç½®:
==================================================
hidden_size: 512
num_layers: 8
epochs: 3
batch_size: 4
...
==================================================
ä½¿ç”¨è®¾å¤‡: cuda:0
åˆå§‹åŒ–æ¨¡å‹...
å¼€å§‹è®­ç»ƒ 3 ä¸ªepoch...
Epoch [1/3], Step [10/100], Loss: 8.7634, Avg Loss: 8.8521
Epoch [1/3], Step [20/100], Loss: 8.5234, Avg Loss: 8.6234
...
æ¨¡å‹å·²ä¿å­˜åˆ°: ./output/model_epoch_1.pth
è®­ç»ƒå®Œæˆ! æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: ./output/model_final.pth
```

## è®¸å¯è¯

ç»§æ‰¿è‡ªåŸ MiniMind é¡¹ç›®
